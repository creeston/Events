{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove stop words\n",
    "# LTSM + Dense + Dense\n",
    "# No embeddings (50 dims)\n",
    "# Val: 0.76 after 13 epochs\n",
    "\n",
    "\n",
    "# Remove stop words\n",
    "# LSTM + Dense + Dense\n",
    "# Embeddings (ruscorpora_upos_cbow_300_20_2019 #180)\n",
    "# acc: 0.9433 - val_acc: 0.8199 after 10 epochs\n",
    "\n",
    "# Remove stop words\n",
    "# LSTM + Dense + Dense\n",
    "# Embeddings (ruwikiruscorpora_upos_skipgram_300_2_2019 #182)\n",
    "# acc: 0.9413 - val_loss: 1.0506 - val_acc: 0.8571\n",
    "\n",
    "# Embeddings (tayga_none_fasttextcbow_300_10_2019 #187)\n",
    "# loss: 0.2405 - acc: 0.9440 - val_loss: 1.7026 - val_acc: 0.7391\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import json\n",
    "import random\n",
    "import urlextract\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import Model, Input, Sequential, initializers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Layer, InputSpec, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter, defaultdict\n",
    "from allennlp.commands.elmo import ElmoEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2207\n"
     ]
    }
   ],
   "source": [
    "# load data from doccano\n",
    "raw_data = []\n",
    "with open(\"C:\\\\Projects\\\\Research\\\\Events\\\\notebooks\\\\file.json\", \"r+\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        raw_data.append(json.loads(line.strip()))\n",
    "\n",
    "data = [(d['text'], [a['label'] for a in d['annotations']]) for d in raw_data]\n",
    "data = [d for d in data if len(d[1]) > 0]\n",
    "\n",
    "labels = sorted(list(set([item for sublist in [d[1] for d in data] for item in sublist])))\n",
    "label_names = [\n",
    "    \"Вечеринка\", \"Выставка\", \"Конкурс\", \"Конференция\", \"Концерт\", \"Лекция\", \"Спектакль\", \"Фестиваль\", \"Шоу\", \n",
    "    \"Встреча\", \"Презентация\", \"Прием заявок\", \"Просмотр\", \"Экскурсия\", \"Ярмарка\", \"Игра\", \"Стендап\",  \n",
    "    \"Дегустация\", \"Спортивное мероприятие\", \"Концертная программа\", \"Курсы\", \"Обсуждение\", \"Занятие\"]\n",
    "\n",
    "\n",
    "data = [(d[0], [label_names[labels.index(l)] for l in d[1]]) for d in data]\n",
    "print(len(label_names) == len(labels))\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "with open(\"C:\\\\Projects\\\\Research\\\\Events\\\\data\\\\training\\\\type_data.json\", \"w+\", encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data for training\n",
    "with open(\"C:\\\\Projects\\\\Research\\\\Events\\\\data\\\\training\\\\type_data.json\", \"r+\", encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "label_mapping = {\n",
    "    \"Встреча\": [\"training\"],\n",
    "    \"Курсы\": [\"training\"],\n",
    "    \"Презентация\": [\"lecture\"],\n",
    "    \"Прием заявок\": [],\n",
    "    \"Конференция\": [\"lecture\"],\n",
    "    \"Обсуждение\": [\"training\"],\n",
    "    'Вечеринка': [\"party\"],\n",
    "    'Выставка': ['exhibition'],\n",
    "    'Занятие': ['training'],\n",
    "    'Игра': ['game'],\n",
    "    'Концерт': ['concert'],\n",
    "    'Концертная программа': ['concert_programm'],\n",
    "    'Лекция': ['lecture'],\n",
    "    'Просмотр': ['view'],\n",
    "    'Спектакль': ['theatre'],\n",
    "    'Спортивное мероприятие': ['sport'],\n",
    "    'Стендап': ['standup'],\n",
    "    'Фестиваль': ['festival'],\n",
    "    'Шоу': ['show'],\n",
    "    'Экскурсия': ['tour'],\n",
    "    'Ярмарка': ['market'],\n",
    "    'Дегустация': ['food'],\n",
    "    'Конкурс': ['compete']\n",
    "}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    event = data[i]\n",
    "    if len(event[1]) == 0:\n",
    "        continue\n",
    "    mapped_events = []\n",
    "    for j in range(len(event[1])):\n",
    "        label = event[1][j]\n",
    "        if label in label_mapping:\n",
    "            mapped_events.extend(label_mapping[label])\n",
    "        else:\n",
    "            mapped_events.append(label)\n",
    "            \n",
    "    event[1] = list(set(mapped_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['compete',\n",
       " 'concert',\n",
       " 'concert_programm',\n",
       " 'exhibition',\n",
       " 'festival',\n",
       " 'food',\n",
       " 'free',\n",
       " 'game',\n",
       " 'lecture',\n",
       " 'market',\n",
       " 'online',\n",
       " 'open air',\n",
       " 'party',\n",
       " 'show',\n",
       " 'sport',\n",
       " 'standup',\n",
       " 'theatre',\n",
       " 'tour',\n",
       " 'training',\n",
       " 'view']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(list(set([item for sublist in [d[1] for d in data] for item in sublist])))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concert_programm': 501,\n",
       " 'theatre': 553,\n",
       " 'view': 501,\n",
       " 'training': 506,\n",
       " 'concert': 541,\n",
       " 'show': 514,\n",
       " 'standup': 450,\n",
       " 'lecture': 504,\n",
       " 'game': 500,\n",
       " 'festival': 550,\n",
       " 'exhibition': 520,\n",
       " 'tour': 510,\n",
       " 'compete': 140,\n",
       " 'food': 110,\n",
       " 'party': 521,\n",
       " 'sport': 503,\n",
       " 'market': 506,\n",
       " 'free': 1387,\n",
       " 'open air': 197,\n",
       " 'online': 35}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_counts = {}\n",
    "for text, text_labels in data:\n",
    "    for label in text_labels:\n",
    "        if label not in labels_counts:\n",
    "            labels_counts[label] = 0\n",
    "        labels_counts[label] += 1\n",
    "labels_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment classes\n",
    "with open(\"augment_events.json\", \"r+\", encoding='utf-8') as f:\n",
    "    augment_events = json.load(f)\n",
    "    random.shuffle(augment_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_length = 500\n",
    "for event in augment_events:\n",
    "    event_types = event['types']\n",
    "    for event_type in event_types:\n",
    "        if event_type not in labels_counts:\n",
    "            continue\n",
    "        if labels_counts[event_type] < class_length:\n",
    "            text = event['title'] + event['description']\n",
    "            data.append((text, event_types))\n",
    "            for event_type in event_types:\n",
    "                if event_type not in labels_counts:\n",
    "                    continue\n",
    "                labels_counts[event_type] += 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Задаётесь вопросами, куда пригласить возлюблённую или какой подарок сделать подруге на 8 Марта? Приходите в NewPlaceMoscow! Стендап-комики порадуют выступлениями, а организаторы — сюрпризами. Можно не сомневаться: участники популярных телепроектов сделают вечер ярким.\\n\\nв Instagram https://www.instagram.com/newplacekafe/ .\\n',\n",
       " ['standup'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save created dataset as doccano doc\n",
    "with open(\"event_for_training.jsonl\", \"w+\", encoding='utf-8') as f:\n",
    "    random.shuffle(data)\n",
    "    for text, labels in data:\n",
    "        if len(text) < 10:\n",
    "            continue\n",
    "        if len(labels) == 0:\n",
    "            labels = [\"-\"]\n",
    "        j = json.dumps({\"text\": text.strip(), \"labels\": list(set(labels))}, ensure_ascii=False)\n",
    "        f.write(j + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.url_extractor = urlextract.URLExtract()\n",
    "        self.tag_regex = re.compile(r\"<[^>]*>\")\n",
    "        self.email_regex = re.compile(r\"[^\\s]+@[^\\s]+\")\n",
    "        self.number_regex = re.compile(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?')\n",
    "        self.dollar_regex = re.compile(r\"[$]+\")\n",
    "        self.spaces_regex = re.compile(r\"\\s+\")\n",
    "        self.special_chars = [\n",
    "            \"<\", \"[\", \"]\", \"`\", \"^\", \">\", \"+\", \"?\", \"!\", \"'\", \".\", \",\", \":\",\n",
    "            \"*\", \"%\", \"#\", \"_\", \"=\", \"-\", \"&\", '/', '\\\\', '(', ')', \";\", \"\\\"\", \"«\", \"»\", \"|\", \"•\", \"—\", \"–\", \"●\", \"►\", \"\\n\",\n",
    "            \"@\"\n",
    "        ]\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\\\xa', ' ')\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.replace_urls(text)\n",
    "        text = self.replace_emails(text)\n",
    "        text = self.replace_numbers(text)\n",
    "        text = self.replace_dollar_signs(text)\n",
    "        return text\n",
    "\n",
    "    def preprocess_to_sentences(self, text):\n",
    "        text = self.preprocess_text(text)\n",
    "        sentences = self.split_by_sentences(text)\n",
    "        sentences = self.remove_special_characters(sentences)\n",
    "        sentences = list(self.split_by_tokens(sentences))\n",
    "        sentences = list(self.remove_stop_words(sentences))\n",
    "        return sentences\n",
    "    \n",
    "    def preprocess_to_tokens(self, text):\n",
    "        text = self.preprocess_text(text)\n",
    "        sentences = self.remove_special_characters([text])\n",
    "        sentences = list(self.split_by_tokens(sentences))\n",
    "        sentences = list(self.remove_stop_words(sentences))\n",
    "        return sentences[0]\n",
    "    \n",
    "    def split_by_tokens(self, sentences):\n",
    "        for sentence in sentences:\n",
    "            yield [morph.parse(t.lower().strip())[0].normal_form for t in word_tokenize(sentence)]\n",
    "    \n",
    "    def split_by_sentences(self, text):\n",
    "        lines = text.split('\\n')\n",
    "        sentences = []\n",
    "        for line in lines:\n",
    "            sentences.extend(sent_tokenize(line))\n",
    "        return sentences\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        text = self.tag_regex.sub(\" \", text).split(\" \")\n",
    "        text = filter(len, text)\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "\n",
    "    def replace_urls(self, text):\n",
    "        urls = list(set(self.url_extractor.find_urls(text)))\n",
    "        urls.sort(key=lambda u: len(u), reverse=True)\n",
    "        for url in urls:\n",
    "            text = text.replace(url, \" httpaddr \")\n",
    "        return text\n",
    "\n",
    "    def replace_emails(self, text):\n",
    "        return self.email_regex.sub(\" emailaddr \", text)\n",
    "    \n",
    "    def replace_numbers(self, text):\n",
    "        return self.number_regex.sub(\" number \", text)\n",
    "\n",
    "    def replace_dollar_signs(self, text):\n",
    "        return self.dollar_regex.sub(\" dollar \", text)\n",
    "\n",
    "    def remove_special_characters(self, sentences):\n",
    "        for i in range(len(sentences)):\n",
    "            for char in self.special_chars:\n",
    "                sentences[i] = sentences[i].replace(str(char), \" \")\n",
    "        return sentences\n",
    "    \n",
    "    def remove_stop_words(self, sentences):\n",
    "        for tokens in sentences:\n",
    "            yield [token for token in tokens if token not in self.stop_words]\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def tokenize(text):\n",
    "    return preprocessor.preprocess_to_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(tokenize(d[0]), d[1]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "\n",
    "def load_embeddings(filename):\n",
    "    vocab = []\n",
    "    idx2word = [\"<PAD>\", \"<UNK>\"]\n",
    "    word2idx = defaultdict(lambda: 1)\n",
    "    word2idx[\"<PAD>\"] = 0\n",
    "    word2idx[\"<UNK>\"] = 1\n",
    "    embeddings = []\n",
    "    \n",
    "    with open(filename, \"r+\", encoding=\"utf-8\") as f:\n",
    "        vocab_size, embedding_dim = f.readline().strip().split(\" \")\n",
    "        embedding_dim = int(embedding_dim)\n",
    "        embeddings = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n",
    "        for line in f:\n",
    "            word_pos, *vector = line.strip().split(\" \")\n",
    "            word, pos = word_pos.split(\"_\")\n",
    "            vector = np.array([float(v) for v in vector])\n",
    "            idx2word.append(word)\n",
    "            word2idx[word] = len(idx2word) - 1\n",
    "            embeddings.append(vector)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    return idx2word, word2idx, embeddings\n",
    "\n",
    "\n",
    "idx2word, word2idx, embeddings = load_embeddings(\"static embeddings\\\\182\\\\model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vocab\n",
    "vocab = sorted(list(set([item for sublist in [d[0] for d in data] for item in sublist])))\n",
    "\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "idx2word = special_tokens\n",
    "word2idx = defaultdict(lambda: 1)\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "for word in vocab:\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = len(idx2word) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAASBElEQVR4nO3df6xfd13H8efLFsoGLmzZ3VJ6G28xDdotGuCmTkmIcZg1jtD9M1MiUHWmkVTFX8EWEvmrSVGCSuKWNAMpca42Y2aNC8JSMMQEN+8GuHWlrtLZXlbWq/hjYlJsffvH92C+3n1v2+/3e/stvZ/nI7n5nvM+n/M9n3O2+zqnn/P9npuqQpLUhu+70h2QJE2OoS9JDTH0Jakhhr4kNcTQl6SGrL7SHbiYG2+8sWZmZq50NyTpqvLkk0/+c1VNLa5/z4f+zMwMc3NzV7obknRVSfJPg+oO70hSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkO+57+R+71sZtejA+vP771zwj2RpEvjlb4kNcTQl6SGGPqS1BBDX5IactHQT/KJJGeSPDNg2W8nqSQ39tV2Jzme5FiSO/rqb07ydLfsY0myfLshSboUl3Kl/0lgy+JikvXATwMn+2qbgG3ALd069yZZ1S2+D9gBbOx+XvaekqTL66KhX1VfBL41YNEfAO8Hqq+2FThQVWer6gRwHNicZC1wXVV9qaoK+BRw19i9lyQNZaQx/STvAL5RVV9dtGgdcKpvfr6rreumF9eXev8dSeaSzC0sLIzSRUnSAEOHfpJrgQ8Cvzto8YBaXaA+UFXtq6rZqpqdmnrZn3iUJI1olG/k/iCwAfhqdy92GngqyWZ6V/Dr+9pOAy909ekBdUnSBA19pV9VT1fVTVU1U1Uz9AL9TVX1TeAQsC3JmiQb6N2wfaKqTgMvJbmt+9TOe4BHlm83JEmX4lI+svkg8CXgDUnmk9yzVNuqOgIcBJ4F/grYWVXnu8XvBe6nd3P3H4HPjNl3SdKQLjq8U1XvvMjymUXze4A9A9rNAbcO2T9J0jLyG7mS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNGeWPqDRnZtejV7oLkrQsvNKXpIYY+pLUEENfkhpi6EtSQwx9SWrIpfxh9E8kOZPkmb7a7yf5WpK/T/IXSV7bt2x3kuNJjiW5o6/+5iRPd8s+liTLvzuSpAu5lCv9TwJbFtUeA26tqh8B/gHYDZBkE7ANuKVb594kq7p17gN2ABu7n8XvKUm6zC4a+lX1ReBbi2qfq6pz3ezfAtPd9FbgQFWdraoTwHFgc5K1wHVV9aWqKuBTwF3LtROSpEuzHGP6vwh8ppteB5zqWzbf1dZ104vrkqQJGiv0k3wQOAc88N3SgGZ1gfpS77sjyVySuYWFhXG6KEnqM3LoJ9kOvB34uW7IBnpX8Ov7mk0DL3T16QH1gapqX1XNVtXs1NTUqF2UJC0yUugn2QL8DvCOqvqvvkWHgG1J1iTZQO+G7RNVdRp4Kclt3ad23gM8MmbfJUlDuugD15I8CPwkcGOSeeBD9D6tswZ4rPvk5d9W1S9X1ZEkB4Fn6Q377Kyq891bvZfeJ4GuoXcP4DNIkibqoqFfVe8cUP74BdrvAfYMqM8Btw7VO0nSsvIbuZLUEENfkhpi6EtSQwx9SWqIoS9JDfFv5F4GS/1N3ef33jnhnkjS/+eVviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqyEVDP8knkpxJ8kxf7YYkjyV5rnu9vm/Z7iTHkxxLckdf/c1Jnu6WfSxJln93JEkXcilX+p8Etiyq7QIOV9VG4HA3T5JNwDbglm6de5Os6ta5D9gBbOx+Fr+nJOkyu2joV9UXgW8tKm8F9nfT+4G7+uoHqupsVZ0AjgObk6wFrquqL1VVAZ/qW0eSNCGjjunfXFWnAbrXm7r6OuBUX7v5rraum15cHyjJjiRzSeYWFhZG7KIkabHlvpE7aJy+LlAfqKr2VdVsVc1OTU0tW+ckqXWjhv6L3ZAN3euZrj4PrO9rNw280NWnB9QlSRM0augfArZ309uBR/rq25KsSbKB3g3bJ7ohoJeS3NZ9auc9fetIkiZk9cUaJHkQ+EngxiTzwIeAvcDBJPcAJ4G7AarqSJKDwLPAOWBnVZ3v3uq99D4JdA3wme5HkjRBFw39qnrnEotuX6L9HmDPgPoccOtQvZMkLSu/kStJDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZKzQT/IbSY4keSbJg0leleSGJI8lea57vb6v/e4kx5McS3LH+N2XJA1j5NBPsg74NWC2qm4FVgHbgF3A4araCBzu5kmyqVt+C7AFuDfJqvG6L0kaxrjDO6uBa5KsBq4FXgC2Avu75fuBu7rprcCBqjpbVSeA48DmMbcvSRrCyKFfVd8APgKcBE4D/15VnwNurqrTXZvTwE3dKuuAU31vMd/VXibJjiRzSeYWFhZG7aIkaZFxhneup3f1vgF4HfDqJO+60CoDajWoYVXtq6rZqpqdmpoatYuSpEXGGd55G3Ciqhaq6r+Bh4GfAF5Mshagez3TtZ8H1vetP01vOEiSNCHjhP5J4LYk1yYJcDtwFDgEbO/abAce6aYPAduSrEmyAdgIPDHG9iVJQ1o96opV9XiSh4CngHPAl4F9wGuAg0nuoXdiuLtrfyTJQeDZrv3Oqjo/Zv8lSUMYOfQBqupDwIcWlc/Su+of1H4PsGecbUqSRuc3ciWpIYa+JDXE0Jekhow1pr/SzOx69Ep3QZIuK6/0JakhXulP0IX+JfH83jsn2BNJrfJKX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZKzQT/LaJA8l+VqSo0l+PMkNSR5L8lz3en1f+91Jjic5luSO8bsvSRrGuFf6fwT8VVX9EPCjwFFgF3C4qjYCh7t5kmwCtgG3AFuAe5OsGnP7kqQhjBz6Sa4D3gp8HKCqvlNV/wZsBfZ3zfYDd3XTW4EDVXW2qk4Ax4HNo25fkjS8ca70Xw8sAH+S5MtJ7k/yauDmqjoN0L3e1LVfB5zqW3++q71Mkh1J5pLMLSwsjNFFSVK/cUJ/NfAm4L6qeiPwbbqhnCVkQK0GNayqfVU1W1WzU1NTY3RRktRvnNCfB+ar6vFu/iF6J4EXk6wF6F7P9LVf37f+NPDCGNuXJA1p5NCvqm8Cp5K8oSvdDjwLHAK2d7XtwCPd9CFgW5I1STYAG4EnRt2+JGl4q8dc/1eBB5K8Evg68Av0TiQHk9wDnATuBqiqI0kO0jsxnAN2VtX5MbcvSRrCWKFfVV8BZgcsun2J9nuAPeNsU5I0Or+RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIeP+jdyr0syuR690FyTpihg79JOsAuaAb1TV25PcAPw5MAM8D/xsVf1r13Y3cA9wHvi1qvrsuNtfKZY6ET2/984J90TSSrYcwzvvA472ze8CDlfVRuBwN0+STcA24BZgC3Bvd8KQJE3IWKGfZBq4E7i/r7wV2N9N7wfu6qsfqKqzVXUCOA5sHmf7kqThjHul/4fA+4H/6avdXFWnAbrXm7r6OuBUX7v5rvYySXYkmUsyt7CwMGYXJUnfNXLoJ3k7cKaqnrzUVQbUalDDqtpXVbNVNTs1NTVqFyVJi4xzI/ctwDuS/AzwKuC6JH8KvJhkbVWdTrIWONO1nwfW960/DbwwxvYlSUMa+Uq/qnZX1XRVzdC7Qfv5qnoXcAjY3jXbDjzSTR8CtiVZk2QDsBF4YuSeS5KGdjk+p78XOJjkHuAkcDdAVR1JchB4FjgH7Kyq85dh+5KkJSxL6FfVXwN/3U3/C3D7Eu32AHuWY5uSpOH5GAZJaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ25HH8jV8toZtejA+vP771zwj2RtBKMfKWfZH2SLyQ5muRIkvd19RuSPJbkue71+r51dic5nuRYkjuWYwckSZdunOGdc8BvVdUPA7cBO5NsAnYBh6tqI3C4m6dbtg24BdgC3Jtk1TidlyQNZ+TQr6rTVfVUN/0ScBRYB2wF9nfN9gN3ddNbgQNVdbaqTgDHgc2jbl+SNLxluZGbZAZ4I/A4cHNVnYbeiQG4qWu2DjjVt9p8Vxv0fjuSzCWZW1hYWI4uSpJYhhu5SV4DfBr49ar6jyRLNh1Qq0ENq2ofsA9gdnZ2YJtLsdRNUElq1VhX+kleQS/wH6iqh7vyi0nWdsvXAme6+jywvm/1aeCFcbYvSRrOOJ/eCfBx4GhVfbRv0SFgeze9HXikr74tyZokG4CNwBOjbl+SNLxxhnfeArwbeDrJV7raB4C9wMEk9wAngbsBqupIkoPAs/Q++bOzqs6PsX1J0pBGDv2q+hsGj9MD3L7EOnuAPaNuU5I0Hr+Re5Xym7qSRuGzdySpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kN8ctZK4xf2pJ0IV7pS1JDDH1JaojDO41w2EcSGPpagicJaWVyeEeSGmLoS1JDDH1Jaohj+o1bauxe0spk6Gso3uCVrm6GvpbFJP7F4IlFGt/Ex/STbElyLMnxJLsmvX1JatlEr/STrAL+GPhpYB74uySHqurZSfZDV6crdf/Bf2FoJZn08M5m4HhVfR0gyQFgK2Do63vWsCebYU8Sy3ky8wSli5l06K8DTvXNzwM/trhRkh3Ajm72P5McG3I7NwL/PFIPV4bW9x+u4DHIh6/EVl+2bf8f8Bj8wKDipEM/A2r1skLVPmDfyBtJ5qpqdtT1r3at7z94DFrff/AYLGXSN3LngfV989PACxPugyQ1a9Kh/3fAxiQbkrwS2AYcmnAfJKlZEx3eqapzSX4F+CywCvhEVR25DJsaeWhohWh9/8Fj0Pr+g8dgoFS9bEhdkrRC+cA1SWqIoS9JDVlRod/CIx6SrE/yhSRHkxxJ8r6ufkOSx5I8171e37fO7u6YHEtyx5Xr/fJKsirJl5P8ZTffzDFI8tokDyX5Wvf/wo+3tP8ASX6j+x14JsmDSV7V2jEYSVWtiB96N4b/EXg98Ergq8CmK92vy7Cfa4E3ddPfD/wDsAn4PWBXV98FfLib3tQdizXAhu4YrbrS+7FMx+I3gT8D/rKbb+YYAPuBX+qmXwm8trH9XwecAK7p5g8CP9/SMRj1ZyVd6f/fIx6q6jvAdx/xsKJU1emqeqqbfgk4Su8XYCu9IKB7vaub3gocqKqzVXUCOE7vWF3VkkwDdwL395WbOAZJrgPeCnwcoKq+U1X/RiP732c1cE2S1cC19L7z09oxGNpKCv1Bj3hYd4X6MhFJZoA3Ao8DN1fVaeidGICbumYr9bj8IfB+4H/6aq0cg9cDC8CfdMNb9yd5Ne3sP1X1DeAjwEngNPDvVfU5GjoGo1pJoX9Jj3hYKZK8Bvg08OtV9R8XajqgdlUflyRvB85U1ZOXusqA2tV8DFYDbwLuq6o3At+mN5SxlJW2/3Rj9VvpDdW8Dnh1knddaJUBtav6GIxqJYV+M494SPIKeoH/QFU93JVfTLK2W74WONPVV+JxeQvwjiTP0xvG+6kkf0o7x2AemK+qx7v5h+idBFrZf4C3ASeqaqGq/ht4GPgJ2joGI1lJod/EIx6ShN5Y7tGq+mjfokPA9m56O/BIX31bkjVJNgAbgScm1d/Loap2V9V0Vc3Q++/8+ap6F40cg6r6JnAqyRu60u30Hk/exP53TgK3Jbm2+524nd79rZaOwUhWzJ9LrMk94uFKewvwbuDpJF/pah8A9gIHk9xD7xfiboCqOpLkIL1QOAfsrKrzk+/2RLR0DH4VeKC7wPk68Av0LuKa2P+qejzJQ8BT9Pbpy/Qeu/AaGjkGo/IxDJLUkJU0vCNJughDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXkfwHcdSCNPAYECwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# analyze lengths\n",
    "\n",
    "lengths = [len(d[0]) for d in data]\n",
    "plt.hist(lengths, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 550\n",
    "\n",
    "def pad_data(tokens):\n",
    "    idxs = [word2idx[word] for word in tokens]\n",
    "    pad_idx = word2idx[\"<PAD>\"]\n",
    "    if len(idxs) < max_length:\n",
    "        idxs.extend([pad_idx] * (max_length - len(idxs)))\n",
    "    else:\n",
    "        return np.array(idxs[:max_length])\n",
    "    return np.array(idxs)\n",
    "\n",
    "def pad_data_fasttext(tokens, model):\n",
    "    vectors = [model[word] for word in tokens]\n",
    "    dim = len(vectors[0])\n",
    "    pad_vector = np.zeros(dim)\n",
    "    if len(vectors) < max_length:\n",
    "        vectors.extend([pad_vector] * (max_length - len(vectors)))\n",
    "    else:\n",
    "        return np.array(vectors[:max_length])\n",
    "    return np.array(vectors)\n",
    "\n",
    "def pad_data_elmo(sentences, model):\n",
    "    vectors = []\n",
    "    for tokens in sentences: \n",
    "        vectors.extend(elmo.embed_sentence(tokens))\n",
    "    vectors = np.concatenate((vectors[0], vectors[1], vectors[2]), axis=1)\n",
    "    count, dim = vectors.shape\n",
    "    pad_vector = np.zeros(dim)\n",
    "    vectors = list(vectors)\n",
    "    if len(vectors) < max_length:\n",
    "        vectors.extend([pad_vector] * (max_length - len(vectors)))\n",
    "    else:\n",
    "        return np.array(vectors[:max_length])\n",
    "    return np.array(vectors)\n",
    "\n",
    "def labels_to_one_hot(text_labels):\n",
    "    one_hot = np.zeros(len(labels))\n",
    "    for label in text_labels:\n",
    "        one_hot[labels.index(label)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data = []\n",
    "y_train_data = []\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for tokens, token_labels in data:\n",
    "    train_data.append((pad_data(tokens), labels_to_one_hot(token_labels)))\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "val_data = train_data[:289]\n",
    "train_data = train_data[289:]\n",
    "\n",
    "x_train_data = np.array([d[0] for d in train_data])\n",
    "y_train_data = np.array([d[1] for d in train_data])\n",
    "                      \n",
    "x_val_data = np.array([d[0] for d in val_data])\n",
    "y_val_data = np.array([d[1] for d in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 550)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasttext\n",
    "\n",
    "model = FastTextKeyedVectors.load(\"static embeddings\\\\187\\\\model.model\")\n",
    "\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for tokens, token_labels in data:\n",
    "    train_data.append((pad_data_fasttext(tokens, model), labels_to_one_hot(token_labels)))\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "val_data = train_data[:207]\n",
    "train_data = train_data[207:]\n",
    "\n",
    "x_train_data = np.array([d[0] for d in train_data])\n",
    "y_train_data = np.array([d[1] for d in train_data])\n",
    "                      \n",
    "x_val_data = np.array([d[0] for d in val_data])\n",
    "y_val_data = np.array([d[1] for d in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo\n",
    "# long to execute\n",
    "\n",
    "elmo = ElmoEmbedder(options_file='context embeddings\\\\199\\\\options.json', weight_file='context embeddings\\\\199\\\\model.hdf5')\n",
    "\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "\n",
    "train_data = []\n",
    "\n",
    "i = 0\n",
    "for tokens, token_labels in data:\n",
    "    train_data.append((pad_data_elmo(tokens, elmo), labels_to_one_hot(token_labels)))\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "val_data = train_data[:161]\n",
    "train_data = train_data[161:]\n",
    "\n",
    "x_train_data = np.array([d[0] for d in train_data])\n",
    "y_train_data = np.array([d[1] for d in train_data])\n",
    "                      \n",
    "x_val_data = np.array([d[0] for d in val_data])\n",
    "y_val_data = np.array([d[1] for d in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 550, 300)          74694000  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               439296    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 75,431,684\n",
      "Trainable params: 737,684\n",
      "Non-trainable params: 74,694,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Embeddings\n",
    "\n",
    "n_words, embedding_dim = embeddings.shape\n",
    "train_len, length = x_train_data.shape\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=n_words, output_dim=embedding_dim, \n",
    "              weights=[embeddings], trainable=False, mask_zero=True, \n",
    "              input_shape=(length, )),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.1),\n",
    "    Dense(512),\n",
    "    Dropout(0.1),\n",
    "    Dense(256),\n",
    "    Dropout(0.1),\n",
    "    Dense(128),\n",
    "    Dense(len(labels), activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 289 samples\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 1679s 240ms/sample - loss: 2.6767 - acc: 0.3867 - val_loss: 1.9279 - val_acc: 0.5675\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 1436s 205ms/sample - loss: 1.8377 - acc: 0.5957 - val_loss: 1.7413 - val_acc: 0.6159\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 1436s 205ms/sample - loss: 1.5531 - acc: 0.6520 - val_loss: 1.8596 - val_acc: 0.6194\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 1431s 204ms/sample - loss: 1.3247 - acc: 0.6950 - val_loss: 1.8801 - val_acc: 0.6540\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 1434s 205ms/sample - loss: 1.1854 - acc: 0.7209 - val_loss: 1.6692 - val_acc: 0.7024\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 1446s 207ms/sample - loss: 1.0383 - acc: 0.7523 - val_loss: 1.7811 - val_acc: 0.6505\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 1432s 205ms/sample - loss: 0.9363 - acc: 0.7711 - val_loss: 1.8252 - val_acc: 0.7059\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 1439s 206ms/sample - loss: 0.8753 - acc: 0.7836 - val_loss: 1.7632 - val_acc: 0.6955\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 1434s 205ms/sample - loss: 0.8421 - acc: 0.7943 - val_loss: 1.8932 - val_acc: 0.6990\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 1434s 205ms/sample - loss: 0.7739 - acc: 0.8000 - val_loss: 2.2894 - val_acc: 0.6817\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_data, y_train_data, batch_size=32, epochs=10, verbose=1, validation_data=(x_val_data, y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_3 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                5140      \n",
      "=================================================================\n",
      "Total params: 389,396\n",
      "Trainable params: 389,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Fasttext (ELMO) Embeddings\n",
    "\n",
    "_, length, dim = x_train_data.shape\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64), input_shape=(length, dim, )),\n",
    "    Dropout(0.1),\n",
    "    Dense(512),\n",
    "    Dense(256),\n",
    "    Dense(len(labels), activation='sigmoid')\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7082 samples, validate on 207 samples\n",
      "Epoch 1/10\n",
      "7082/7082 [==============================] - 583s 82ms/sample - loss: 2.9272 - acc: 0.3328 - val_loss: 2.2936 - val_acc: 0.4879\n",
      "Epoch 2/10\n",
      "7082/7082 [==============================] - 692s 98ms/sample - loss: 2.1022 - acc: 0.5345 - val_loss: 2.0483 - val_acc: 0.5556\n",
      "Epoch 3/10\n",
      "7082/7082 [==============================] - 685s 97ms/sample - loss: 1.7738 - acc: 0.6062 - val_loss: 1.9868 - val_acc: 0.5700\n",
      "Epoch 4/10\n",
      "7082/7082 [==============================] - 500s 71ms/sample - loss: 1.5497 - acc: 0.6361 - val_loss: 2.0607 - val_acc: 0.5604\n",
      "Epoch 5/10\n",
      "7082/7082 [==============================] - 456s 64ms/sample - loss: 1.4131 - acc: 0.6661 - val_loss: 1.9058 - val_acc: 0.5894\n",
      "Epoch 6/10\n",
      "7082/7082 [==============================] - 458s 65ms/sample - loss: 1.2858 - acc: 0.6994 - val_loss: 2.0057 - val_acc: 0.5700\n",
      "Epoch 7/10\n",
      "7082/7082 [==============================] - 458s 65ms/sample - loss: 1.2045 - acc: 0.7101 - val_loss: 2.0110 - val_acc: 0.6135\n",
      "Epoch 8/10\n",
      "7082/7082 [==============================] - 459s 65ms/sample - loss: 1.1196 - acc: 0.7251 - val_loss: 2.0829 - val_acc: 0.6135\n",
      "Epoch 9/10\n",
      "7082/7082 [==============================] - 482s 68ms/sample - loss: 1.0882 - acc: 0.7319 - val_loss: 2.3052 - val_acc: 0.5990\n",
      "Epoch 10/10\n",
      "7082/7082 [==============================] - 468s 66ms/sample - loss: 1.0357 - acc: 0.7450 - val_loss: 2.3237 - val_acc: 0.6232\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_data, y_train_data, batch_size=batch_size, epochs=10, verbose=1, validation_data=(x_val_data, y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict([x_val_data, i_val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_val_pred[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential, initializers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Layer, InputSpec, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "n_words = len(idx2word)\n",
    "embedding_dim = 50\n",
    "batch_size = 32\n",
    "\n",
    "tokens_input = Input(shape=(max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=n_words, output_dim=embedding_dim)(tokens_input)\n",
    "features = Bidirectional(LSTM(128))(embedding)\n",
    "features = Dropout(0.1)(features)\n",
    "dense = Dense(512)(features)\n",
    "dense = Dense(256)(features)\n",
    "output = Dense(len(labels), activation='sigmoid')(features)\n",
    "\n",
    "model = Model(inputs=[tokens_input], outputs=[output])\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 epoch - max val value\n",
    "model.fit([x_train_data], y_train_data, batch_size=batch_size, epochs=15, verbose=1, validation_data=([x_val_data], y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit([x_train_data], y_train_data, batch_size=batch_size, epochs=5, verbose=1, validation_data=([x_val_data], y_val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
