{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove stop words\n",
    "# LTSM + Dense + Dense\n",
    "# No embeddings (50 dims)\n",
    "# Val: 0.76 after 13 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "raw_data = []\n",
    "with open(\"C:\\\\Projects\\\\Research\\\\Events\\\\notebooks\\\\file.json\", \"r+\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        raw_data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(d['text'], [a['label'] for a in d['annotations']]) for d in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(list(set([item for sublist in [d[1] for d in data] for item in sublist])))\n",
    "label_names = [\n",
    "    \"Вечеринка\", \"Выставка\", \"Интенсив\", \"Квиз\", \"Конкурс\", \"Конференция\", \"Концерт\", \"Лекция\", \"Мастер-класс\",\n",
    "    \"Семинар\", \"Спектакль\", \"Тренинг\", \"Фестиваль\", \"Шоу\", \"Встреча\", \"Презентация\", \"Прием заявок\", \"Просмотр\", \"Экскурсия\",\n",
    "    \"Ярмарка\", \"Автограф-сессия\", \"Квест\", \"Модный показ\", \"Хакатон\", \"Игра\", \"Стендап\", \"Спортивное мероприятие\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "import re\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.url_extractor = urlextract.URLExtract()\n",
    "        self.tag_regex = re.compile(r\"<[^>]*>\")\n",
    "        self.email_regex = re.compile(r\"[^\\s]+@[^\\s]+\")\n",
    "        self.number_regex = re.compile(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?')\n",
    "        self.dollar_regex = re.compile(r\"[$]+\")\n",
    "        self.spaces_regex = re.compile(r\"\\s+\")\n",
    "        self.special_chars = [\n",
    "            \"<\", \"[\", \"]\", \"`\", \"^\", \">\", \"+\", \"?\", \"!\", \"'\", \".\", \",\", \":\",\n",
    "            \"*\", \"%\", \"#\", \"_\", \"=\", \"-\", \"&\", '/', '\\\\', '(', ')', \";\", \"\\\"\", \"«\", \"»\", \"|\", \"•\", \"—\", \"–\", \"●\", \"►\", \"\\n\",\n",
    "            \"@\"\n",
    "        ]\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.replace_urls(text)\n",
    "        text = self.replace_emails(text)\n",
    "        text = self.replace_numbers(text)\n",
    "        text = self.replace_dollar_signs(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = self.remove_stop_words(text)\n",
    "        text = self.spaces_regex.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        text = self.tag_regex.sub(\" \", text).split(\" \")\n",
    "        text = filter(len, text)\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "\n",
    "    def replace_urls(self, text):\n",
    "        urls = list(set(self.url_extractor.find_urls(text)))\n",
    "        urls.sort(key=lambda u: len(u), reverse=True)\n",
    "        for url in urls:\n",
    "            text = text.replace(url, \" httpaddr \")\n",
    "        return text\n",
    "\n",
    "    def replace_emails(self, text):\n",
    "        return self.email_regex.sub(\" emailaddr \", text)\n",
    "    \n",
    "    def replace_numbers(self, text):\n",
    "        return self.number_regex.sub(\" number \", text)\n",
    "\n",
    "    def replace_dollar_signs(self, text):\n",
    "        return self.dollar_regex.sub(\" dollar \", text)\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        for char in self.special_chars:\n",
    "            text = text.replace(str(char), \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_stop_words(self, text):\n",
    "        for word in self.stop_words:\n",
    "            text = text.replace(\" %s \" % word, \" \")\n",
    "        return text\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = preprocessor.preprocess_text(text)\n",
    "    return [morph.parse(t.lower())[0].normal_form for t in word_tokenize(text)]\n",
    "\n",
    "def calculate_tf_idf(documents):\n",
    "    words_freqs = []\n",
    "    for i, document in enumerate(documents):\n",
    "        words_freqs.append({})\n",
    "        token_counts = Counter(document)\n",
    "        total = sum([c for (w, c) in token_counts.items()])\n",
    "        for token, count in token_counts.items():\n",
    "            words_freqs[-1][token] = count / total\n",
    "\n",
    "    words_idf = []\n",
    "    for i, document in enumerate(documents):\n",
    "        words_idf.append({})\n",
    "        freqs = words_freqs[i]\n",
    "        for token, freq in freqs.items():\n",
    "            docs_with_token = len([doc for j, doc in enumerate(documents) if token in words_freqs[j]])\n",
    "            idf = math.log(len(documents) / docs_with_token)\n",
    "            words_idf[-1][token] = freq * idf\n",
    "    return words_idf\n",
    "\n",
    "def get_top(i, k):\n",
    "    return [w for w, c in sorted(words_idf[i].items(), key=lambda p:p[1], reverse=True)[:k]]\n",
    "\n",
    "def get_bottom(i, k):\n",
    "    return [w for w, c in sorted(words_idf[i].items(), key=lambda p:p[1], reverse=False)[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(tokenize(d[0]), d[1]) for d in data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "vocab = []\n",
    "embedding_dim = 300\n",
    "embeddings = [np.zeros(embedding_dim), np.random.rand(embedding_dim)]\n",
    "\n",
    "idx2word = [\"<PAD>\", \"<UNK>\"]\n",
    "word2idx = defaultdict(lambda: 0)\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "with open(\"static embeddings\\\\model.txt\", \"r+\", encoding=\"utf-8\") as f:\n",
    "    f.readline() # skip header\n",
    "    for line in f:\n",
    "        word_pos, *vector = line.strip().split(\" \")\n",
    "        word, pos = word_pos.split(\"_\")\n",
    "        vector = np.array([float(v) for v in vector])\n",
    "        idx2word.append(word)\n",
    "        word2idx[word] = len(idx2word) - 1\n",
    "        embeddings.append(vector)\n",
    "        \n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vocab\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "vocab = sorted(list(set([item for sublist in [d[0] for d in data] for item in sublist])))\n",
    "\n",
    "special_tokens = [\"<PAD>\", \"<UNK>\"]\n",
    "idx2word = special_tokens\n",
    "word2idx = defaultdict(lambda: 0)\n",
    "word2idx[\"<PAD>\"] = 0\n",
    "word2idx[\"<UNK>\"] = 1\n",
    "\n",
    "for word in vocab:\n",
    "    idx2word.append(word)\n",
    "    word2idx[word] = len(idx2word) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths = [len(d[0]) for d in data]\n",
    "plt.hist(lengths, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600\n",
    "import numpy as np\n",
    "\n",
    "def pad_data(tokens, max_length=600):\n",
    "    idxs = [word2idx[word] for word in tokens]\n",
    "    pad_idx = word2idx[\"<PAD>\"]\n",
    "    if len(idxs) < max_length:\n",
    "        idxs.extend([pad_idx] * (max_length - len(idxs)))\n",
    "    else:\n",
    "        return np.array(idxs[:max_length])\n",
    "    return np.array(idxs)\n",
    "\n",
    "def get_tfidf(tokens, document_id, max_length=600):\n",
    "    weights = [words_idf[document_id][token] for token in tokens]\n",
    "    if len(weights) < max_length:\n",
    "        weights.extend([0] * (max_length - len(weights)))\n",
    "    else:\n",
    "        return np.array(weights[:max_length])\n",
    "    return np.array(weights)\n",
    "\n",
    "def labels_to_one_hot(text_labels):\n",
    "    one_hot = np.zeros(len(labels))\n",
    "    for label in text_labels:\n",
    "        one_hot[labels.index(label)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tfidf\n",
    "\n",
    "import random \n",
    "\n",
    "x_train_data = []\n",
    "i_train_data = []\n",
    "y_train_data = []\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for i, tokens in enumerate(tokenized_texts):\n",
    "    train_data.append((pad_data(tokens), get_tfidf(tokens, i), labels_to_one_hot(data[i][1])))\n",
    "\n",
    "random.shuffle(train_data)\n",
    "val_data = train_data[:161]\n",
    "train_data = train_data[161:]\n",
    "\n",
    "x_train_data = np.array([d[0] for d in train_data])\n",
    "i_train_data = np.array([d[1] for d in train_data])\n",
    "y_train_data = np.array([d[2] for d in train_data])\n",
    "                      \n",
    "x_val_data = np.array([d[0] for d in val_data])\n",
    "i_val_data = np.array([d[1] for d in val_data])\n",
    "y_val_data = np.array([d[2] for d in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without tfidf\n",
    "\n",
    "import random \n",
    "\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "\n",
    "train_data = []\n",
    "\n",
    "for i, tokens in enumerate(tokenized_texts):\n",
    "    train_data.append((pad_data(tokens), labels_to_one_hot(data[i][1])))\n",
    "\n",
    "random.shuffle(train_data)\n",
    "\n",
    "val_data = train_data[:161]\n",
    "train_data = train_data[161:]\n",
    "\n",
    "x_train_data = np.array([d[0] for d in train_data])\n",
    "y_train_data = np.array([d[1] for d in train_data])\n",
    "                      \n",
    "x_val_data = np.array([d[0] for d in val_data])\n",
    "y_val_data = np.array([d[1] for d in val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No embeddings\n",
    "# TFIDF\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential, initializers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Layer, InputSpec, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "n_words = len(idx2word)\n",
    "embedding_dim = 50\n",
    "batch_size = 32\n",
    "\n",
    "tokens_input = Input(shape=(max_length,))\n",
    "tf_idf_input = Input(shape=(max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=n_words, output_dim=embedding_dim)(tokens_input)\n",
    "features = Bidirectional(LSTM(64))(embedding)\n",
    "features = Dropout(0.1)(features)\n",
    "features = tf.keras.layers.concatenate([features, tf_idf_input])\n",
    "dense = Dense(512)(features)\n",
    "output = Dense(len(labels), activation='sigmoid')(features)\n",
    "\n",
    "model = Model(inputs=[tokens_input, tf_idf_input], outputs=[output])\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_train_data, i_train_data], y_train_data, batch_size=batch_size, epochs=15, verbose=1, validation_data=([x_val_data, i_val_data], y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential, initializers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Layer, InputSpec, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "n_words = len(idx2word)\n",
    "embedding_dim = 300\n",
    "batch_size = 32\n",
    "\n",
    "tokens_input = Input(shape=(max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=n_words, output_dim=embedding_dim, weights=[embeddings], trainable=False, mask_zero=True)(tokens_input)\n",
    "features = Bidirectional(LSTM(64))(embedding)\n",
    "features = Dropout(0.1)(features)\n",
    "dense = Dense(512)(features)\n",
    "dense = Dense(256)(dense)\n",
    "output = Dense(len(labels), activation='sigmoid')(dense)\n",
    "\n",
    "model = Model(tokens_input, output)\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_data, y_train_data, batch_size=batch_size, epochs=5, verbose=1, validation_data=(x_val_data, y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict([x_val_data, i_val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_val_pred[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Input, Sequential, initializers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Layer, InputSpec, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Add, Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "\n",
    "\n",
    "n_words = len(idx2word)\n",
    "embedding_dim = 50\n",
    "batch_size = 32\n",
    "\n",
    "tokens_input = Input(shape=(max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=n_words, output_dim=embedding_dim)(tokens_input)\n",
    "features = Bidirectional(LSTM(128))(embedding)\n",
    "features = Dropout(0.1)(features)\n",
    "dense = Dense(512)(features)\n",
    "dense = Dense(256)(features)\n",
    "output = Dense(len(labels), activation='sigmoid')(features)\n",
    "\n",
    "model = Model(inputs=[tokens_input], outputs=[output])\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 epoch - max val value\n",
    "model.fit([x_train_data], y_train_data, batch_size=batch_size, epochs=15, verbose=1, validation_data=([x_val_data], y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit([x_train_data], y_train_data, batch_size=batch_size, epochs=5, verbose=1, validation_data=([x_val_data], y_val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
