{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.stem import PorterStemmer\n",
    "import urlextract\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = load_json(\"training\\\\tags.json\")\n",
    "types = load_json(\"training\\\\types.json\")\n",
    "events = load_json(\"data\\\\events.json\")\n",
    "places = load_json(\"data\\\\places.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.url_extractor = urlextract.URLExtract()\n",
    "        self.tag_regex = re.compile(r\"<[^>]*>\")\n",
    "        self.email_regex = re.compile(r\"[^\\s]+@[^\\s]+\")\n",
    "        self.number_regex = re.compile(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?')\n",
    "        self.dollar_regex = re.compile(r\"[$]+\")\n",
    "        self.spaces_regex = re.compile(r\"\\s+\")\n",
    "        self.special_chars = [\n",
    "            \"<\", \"[\", \"^\", \">\", \"+\", \"?\", \"!\", \"'\", \".\", \",\", \":\",\n",
    "            \"*\", \"%\", \"#\", \"_\", \"=\", \"-\", \"&\", '/', '\\\\', '(', ')', \";\", \"\\\"\", \"«\", \"»\", \"|\", \"•\", \"—\", \"–\", \"●\", \"►\", \"\\n\"\n",
    "        ]\n",
    "        self.stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.replace_urls(text)\n",
    "        text = self.replace_emails(text)\n",
    "        text = self.replace_numbers(text)\n",
    "        text = self.replace_dollar_signs(text)\n",
    "        text = self.replace_places(text)\n",
    "        text = self.stem_words(text)\n",
    "        text = self.remove_special_characters(text)\n",
    "        text = self.remove_stop_words(text)\n",
    "        text = self.spaces_regex.sub(' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        text = self.tag_regex.sub(\" \", text).split(\" \")\n",
    "        text = filter(len, text)\n",
    "        text = ' '.join(text)\n",
    "        return text\n",
    "\n",
    "    def replace_urls(self, text):\n",
    "        urls = list(set(self.url_extractor.find_urls(text)))\n",
    "        urls.sort(key=lambda u: len(u), reverse=True)\n",
    "        for url in urls:\n",
    "            text = text.replace(url, \" httpaddr \")\n",
    "        return text\n",
    "\n",
    "    def replace_emails(self, text):\n",
    "        return self.email_regex.sub(\" emailaddr \", text)\n",
    "    \n",
    "    def replace_places(self, text):\n",
    "        for place in places:\n",
    "            text = text.replace(place, \" place \")\n",
    "        return text\n",
    "\n",
    "    def replace_numbers(self, text):\n",
    "        return self.number_regex.sub(\" number \", text)\n",
    "\n",
    "    def replace_dollar_signs(self, text):\n",
    "        return self.dollar_regex.sub(\" dollar \", text)\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        for char in self.special_chars:\n",
    "            text = text.replace(str(char), \" \")\n",
    "        return text\n",
    "    \n",
    "    def remove_stop_words(self, text):\n",
    "        for word in self.stop_words:\n",
    "            text = text.replace(\" %s \" % word, \" \")\n",
    "        return text\n",
    "\n",
    "    def stem_words(self, text):\n",
    "        text = [self.stemmer.stem(token) for token in text.split(\" \")]\n",
    "        text = \" \".join(text)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "def convert_text_to_feature_vector(text):\n",
    "    text = preprocessor.preprocess_text(text)\n",
    "    words = text.split(' ')\n",
    "    feature_vector = np.zeros(len(mapping))\n",
    "    for word in words:\n",
    "        if word in mapping:\n",
    "            feature_vector[mapping[word] - 1] = 1\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'особенность формата конце вечера зрители выбирают самого смешного комика получает денежный приз зрителей вход свободный'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.preprocess_text(tags[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for text, tag in tags:\n",
    "    text = preprocessor.preprocess_text(text)\n",
    "    words = text.split(' ')\n",
    "    word_counts = Counter(words)\n",
    "    vocab.append(word_counts)\n",
    "\n",
    "vocab = sum(vocab, Counter())\n",
    "most_common = vocab.most_common(2000)\n",
    "vocab = []\n",
    "for (k, v) in most_common:\n",
    "    vocab.append(k)\n",
    "\n",
    "vocab = [(index + 1, word) for index, word in enumerate(sorted(vocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "with open(\"training\\\\vocab\", 'w+', encoding=\"utf-8\") as f:\n",
    "    for index, word in vocab:\n",
    "        f.write(\"%s\\t%s\\n\" % (index, word))\n",
    "        mapping[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = [t[1] for t in tags]\n",
    "tag_data = sorted(list(set([t.lower() for tt in tag_data for t in tt])))\n",
    "tag_mapping = {}\n",
    "for i, t in enumerate(tag_data):\n",
    "    tag_mapping[t] = i\n",
    "\n",
    "def convert_tags_to_vector(tag_list):\n",
    "    v = np.zeros(len(tag_mapping))\n",
    "    for t in tag_list:\n",
    "        t = t.lower()\n",
    "        v[tag_mapping[t]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_data = [t[1] for t in types]\n",
    "type_data = sorted(list(set([t.lower() for tt in type_data for t in tt])))\n",
    "type_mapping = {}\n",
    "for i, t in enumerate(type_data):\n",
    "    type_mapping[t] = i\n",
    "\n",
    "def convert_type_to_vector(type_value):\n",
    "    v = np.zeros(len(type_mapping))\n",
    "    for t in type_value:\n",
    "        t = t.lower()\n",
    "        v[type_mapping[t]] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for text, tag in tags:\n",
    "    feature_vector = convert_text_to_feature_vector(text, mapping)\n",
    "    tag_vector = convert_tags_to_vector(tag, tag_mapping)\n",
    "    x.append(feature_vector)\n",
    "    y.append(tag_vector)\n",
    "    \n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 'dj'\n",
      "0.9980314960629921\n",
      "Tag 'handmade'\n",
      "1.0\n",
      "Tag 'open air'\n",
      "1.0\n",
      "Tag 'авторская песня'\n",
      "1.0\n",
      "Tag 'аксессуары'\n",
      "1.0\n",
      "Tag 'активный отдых'\n",
      "1.0\n",
      "Tag 'акустика'\n",
      "1.0\n",
      "Tag 'альтернатива'\n",
      "1.0\n",
      "Tag 'балет'\n",
      "1.0\n",
      "Tag 'бардовская песня'\n",
      "1.0\n",
      "Tag 'баян'\n",
      "1.0\n",
      "Tag 'бег'\n",
      "1.0\n",
      "Tag 'бесплатно'\n",
      "0.9990157480314961\n",
      "Tag 'биатлон'\n",
      "1.0\n",
      "Tag 'блюз'\n",
      "1.0\n",
      "Tag 'боевые искусства'\n",
      "1.0\n",
      "Tag 'велосипед'\n",
      "1.0\n",
      "Tag 'водевиль'\n",
      "1.0\n",
      "Tag 'вокал'\n",
      "1.0\n",
      "Tag 'волонтерство'\n",
      "1.0\n",
      "Tag 'гастрономия'\n",
      "1.0\n",
      "Tag 'гонка'\n",
      "1.0\n",
      "Tag 'гранж'\n",
      "1.0\n",
      "Tag 'графика'\n",
      "1.0\n",
      "Tag 'детский'\n",
      "1.0\n",
      "Tag 'джаз'\n",
      "1.0\n",
      "Tag 'дизайн'\n",
      "1.0\n",
      "Tag 'для свиданий'\n",
      "1.0\n",
      "Tag 'драма'\n",
      "1.0\n",
      "Tag 'живопись'\n",
      "1.0\n",
      "Tag 'животные'\n",
      "1.0\n",
      "Tag 'игрушки'\n",
      "1.0\n",
      "Tag 'инди-поп'\n",
      "1.0\n",
      "Tag 'инди-рок'\n",
      "1.0\n",
      "Tag 'интерактивная'\n",
      "1.0\n",
      "Tag 'интерьер'\n",
      "1.0\n",
      "Tag 'искусство'\n",
      "1.0\n",
      "Tag 'история'\n",
      "1.0\n",
      "Tag 'йога'\n",
      "1.0\n",
      "Tag 'кавер-бэнд'\n",
      "1.0\n",
      "Tag 'кантри'\n",
      "1.0\n",
      "Tag 'кикер'\n",
      "1.0\n",
      "Tag 'классика'\n",
      "1.0\n",
      "Tag 'книги'\n",
      "1.0\n",
      "Tag 'комедия'\n",
      "1.0\n",
      "Tag 'кукольный'\n",
      "1.0\n",
      "Tag 'кулинария'\n",
      "1.0\n",
      "Tag 'лаунж'\n",
      "1.0\n",
      "Tag 'литературно-документальный'\n",
      "1.0\n",
      "Tag 'личность'\n",
      "1.0\n",
      "Tag 'магия'\n",
      "1.0\n",
      "Tag 'мелодрама'\n",
      "1.0\n",
      "Tag 'метал'\n",
      "1.0\n",
      "Tag 'моноспектакль'\n",
      "1.0\n",
      "Tag 'музыка'\n",
      "1.0\n",
      "Tag 'мюзикл'\n",
      "1.0\n",
      "Tag 'новый год'\n",
      "1.0\n",
      "Tag 'образование'\n",
      "1.0\n",
      "Tag 'образовательное'\n",
      "1.0\n",
      "Tag 'общество'\n",
      "1.0\n",
      "Tag 'опера'\n",
      "1.0\n",
      "Tag 'оперетта'\n",
      "1.0\n",
      "Tag 'оружие'\n",
      "1.0\n",
      "Tag 'панк'\n",
      "1.0\n",
      "Tag 'панк-рок'\n",
      "1.0\n",
      "Tag 'пауэр-метал'\n",
      "1.0\n",
      "Tag 'персональный'\n",
      "1.0\n",
      "Tag 'поделки'\n",
      "1.0\n",
      "Tag 'поп'\n",
      "0.9970472440944882\n",
      "Tag 'поп-панк'\n",
      "1.0\n",
      "Tag 'поп-рок'\n",
      "1.0\n",
      "Tag 'пост-панк'\n",
      "1.0\n",
      "Tag 'премьера'\n",
      "1.0\n",
      "Tag 'природа и экология'\n",
      "1.0\n",
      "Tag 'психология'\n",
      "1.0\n",
      "Tag 'развлекательный'\n",
      "1.0\n",
      "Tag 'регги'\n",
      "1.0\n",
      "Tag 'ретро'\n",
      "0.9990157480314961\n",
      "Tag 'родительство'\n",
      "1.0\n",
      "Tag 'рок'\n",
      "0.9990157480314961\n",
      "Tag 'романс'\n",
      "1.0\n",
      "Tag 'русский рок'\n",
      "1.0\n",
      "Tag 'рыбки'\n",
      "1.0\n",
      "Tag 'рэп'\n",
      "0.9990157480314961\n",
      "Tag 'саксофон'\n",
      "1.0\n",
      "Tag 'ска'\n",
      "1.0\n",
      "Tag 'скрипка'\n",
      "1.0\n",
      "Tag 'смешанные единоборства'\n",
      "1.0\n",
      "Tag 'сноуборд'\n",
      "1.0\n",
      "Tag 'соул'\n",
      "1.0\n",
      "Tag 'спорт'\n",
      "1.0\n",
      "Tag 'строительный'\n",
      "1.0\n",
      "Tag 'танцы'\n",
      "0.9990157480314961\n",
      "Tag 'творческий вечер'\n",
      "1.0\n",
      "Tag 'теннис'\n",
      "1.0\n",
      "Tag 'трагедия'\n",
      "1.0\n",
      "Tag 'трагикомедия'\n",
      "1.0\n",
      "Tag 'трибьют'\n",
      "1.0\n",
      "Tag 'трип-хоп'\n",
      "1.0\n",
      "Tag 'туризм'\n",
      "1.0\n",
      "Tag 'фанк'\n",
      "1.0\n",
      "Tag 'фолк'\n",
      "1.0\n",
      "Tag 'фортепиано'\n",
      "1.0\n",
      "Tag 'фото'\n",
      "1.0\n",
      "Tag 'футбол'\n",
      "1.0\n",
      "Tag 'фьюжн'\n",
      "1.0\n",
      "Tag 'хардкор'\n",
      "1.0\n",
      "Tag 'хеви-метал'\n",
      "1.0\n",
      "Tag 'хоккей'\n",
      "1.0\n",
      "Tag 'хоровая музыка'\n",
      "1.0\n",
      "Tag 'художественный'\n",
      "1.0\n",
      "Tag 'цимбалы'\n",
      "1.0\n",
      "Tag 'шансон'\n",
      "1.0\n",
      "Tag 'электронная музыка'\n",
      "0.9990157480314961\n",
      "Tag 'эстрада'\n",
      "1.0\n",
      "Tag 'юмор'\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "for tag, tag_id in tag_mapping.items(): \n",
    "    print(\"Tag '%s'\" % tag)\n",
    "    y_train = []\n",
    "    for v in y:\n",
    "        y_train.append(v[tag_id])\n",
    "    y_train = np.array(y_train)\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(x, y_train)\n",
    "    print(clf.score(x, y_train))\n",
    "    classifiers.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tags(text):\n",
    "    text = preprocessor.preprocess_text(text)\n",
    "    vector = convert_text_to_feature_vector(text, mapping)\n",
    "    result = []\n",
    "    for i, clf in enumerate(classifiers):\n",
    "        prediction = clf.predict([vector])\n",
    "        if prediction[0] == 1:\n",
    "            result.append(tag_data[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327\n",
      "1319\n",
      "0.9939713639788997\n"
     ]
    }
   ],
   "source": [
    "actual = 0\n",
    "predicted = 0\n",
    "for text, t in tags:\n",
    "    p = set(detect_tags(text))\n",
    "    t = set(v.lower() for v in t)\n",
    "    actual += len(t)\n",
    "    predicted += len(p & t)\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "print(predicted / actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for text, type_value in types:\n",
    "    feature_vector = convert_text_to_feature_vector(text)\n",
    "    type_vector = convert_type_to_vector(type_value)\n",
    "    x.append(feature_vector)\n",
    "    y.append(type_vector)\n",
    "    \n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'автограф-сессия'\n",
      "1.0\n",
      "Type 'вечеринка'\n",
      "0.9985590778097982\n",
      "Type 'воркшоп'\n",
      "1.0\n",
      "Type 'выставка'\n",
      "1.0\n",
      "Type 'интенсив'\n",
      "1.0\n",
      "Type 'квест'\n",
      "1.0\n",
      "Type 'квиз'\n",
      "1.0\n",
      "Type 'кино'\n",
      "1.0\n",
      "Type 'конкурс'\n",
      "1.0\n",
      "Type 'конференция'\n",
      "1.0\n",
      "Type 'концерт'\n",
      "0.989193083573487\n",
      "Type 'лекция'\n",
      "1.0\n",
      "Type 'литературный вечер'\n",
      "1.0\n",
      "Type 'мастер-класс'\n",
      "1.0\n",
      "Type 'митап'\n",
      "1.0\n",
      "Type 'модный показ'\n",
      "1.0\n",
      "Type 'праздник'\n",
      "1.0\n",
      "Type 'презентация'\n",
      "1.0\n",
      "Type 'прием заявок'\n",
      "1.0\n",
      "Type 'семинар'\n",
      "1.0\n",
      "Type 'спектакль'\n",
      "0.9920749279538905\n",
      "Type 'творческая встреча'\n",
      "1.0\n",
      "Type 'тренинг'\n",
      "1.0\n",
      "Type 'турнир'\n",
      "1.0\n",
      "Type 'фестиваль'\n",
      "1.0\n",
      "Type 'форум'\n",
      "1.0\n",
      "Type 'хакатон'\n",
      "1.0\n",
      "Type 'шоу'\n",
      "1.0\n",
      "Type 'экскурсия'\n",
      "1.0\n",
      "Type 'ярмарка'\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "type_classifiers = []\n",
    "for type_value, type_id in type_mapping.items(): \n",
    "    print(\"Type '%s'\" % type_value)\n",
    "    y_train = []\n",
    "    for v in y:\n",
    "        y_train.append(v[type_id])\n",
    "    y_train = np.array(y_train)\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(x, y_train)\n",
    "    print(clf.score(x, y_train))\n",
    "    type_classifiers.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_types(text):\n",
    "    text = preprocessor.preprocess_text(text)\n",
    "    vector = convert_text_to_feature_vector(text)\n",
    "    result = []\n",
    "    for i, clf in enumerate(type_classifiers):\n",
    "        prediction = clf.predict([vector])\n",
    "        if prediction[0] == 1:\n",
    "            result.append(type_data[i])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n",
      "1446\n",
      "0.9904109589041096\n"
     ]
    }
   ],
   "source": [
    "actual = 0\n",
    "predicted = 0\n",
    "for text, t in types:\n",
    "    p = set(detect_types(text))\n",
    "    t = set(v.lower() for v in t)\n",
    "    actual += len(t)\n",
    "    predicted += len(p & t)\n",
    "\n",
    "print(actual)\n",
    "print(predicted)\n",
    "print(predicted / actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from telethon import TelegramClient, events, sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your phone (or bot token): +375445822205\n",
      "Please enter the code you received: 52883\n",
      "Signed in successfully as Родион\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<telethon.client.telegramclient.TelegramClient at 0x28cdb2e49c8>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_id = 747366\n",
    "api_hash = '9810a745cd9de2ac266e15d6152d99f4'\n",
    "\n",
    "client = TelegramClient('session_name_2', api_id, api_hash)\n",
    "await client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telegram\n",
      "Come here, Minsk\n",
      "Берг\n",
      "Родион\n",
      "Vlad Birukov\n",
      "Вероничка\n",
      "Тананыч\n",
      "KSiS OBOI 85864(1/2) (only students)\n",
      "Ярмола\n",
      "aaaaaaaaaaa\n",
      "Dmitry Chabanenko\n"
     ]
    }
   ],
   "source": [
    "async for i in client.iter_dialogs():\n",
    "    print(i.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = await client.get_entity('Come here, Minsk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = list(set(open('data/events', encoding=\"utf-8\").readlines()))\n",
    "not_events = list(set(open('data/not_events', encoding=\"utf-8\").readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_vectors = [convert_text_to_feature_vector(event) for event in events]\n",
    "not_event_vectors = [convert_text_to_feature_vector(not_event) for not_event in not_events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_events = np.zeros(len(event_vectors))\n",
    "y_train_events.fill(1)\n",
    "y_train_not_events = np.zeros(len(not_event_vectors))\n",
    "\n",
    "x_train = np.concatenate((event_vectors, not_event_vectors))\n",
    "y_train = np.concatenate((y_train_events, y_train_not_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': [0.1, 0.5, 1, 1.5], 'kernel': ('linear', 'rbf')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 0.5, 1, 1.5]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(x_train, y_train)\n",
    "#    print(\"Training Accuracy:\", (clf.score(x_train, y_train)) * 100, \"%\")\n",
    "#    print(\"For c=\", c, \" Testing Accuracy:\", (clf.score(x_test, y_test)) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"тематика выставки мебель жилых зон загородных пространств офисов сектора horeca мебель премиум класса мебель предметы интерьера натурального дерева фурнитура материалы комплектующие инновации технологические решения дызайнпрастор дизайн интерьера декор пространства современные отделочные материалы конструкции дизайнерские решения жилых офисных общественных помещений рамках деловой программы участники проанонсируют новинки расскажут успешных кейсах поделятся опытом обсудят специфику ведения бизнеса современных условиях компаний работающих мебельном рынке будут организованы семинары маркетингу эффективных каналах рекламы продажам посетителей ждет ежегодный весенний дизайн amp декор форум площадка коммуникаций дизайнеров интерьера декораторов бесплатные дизайн консультации оформлению офисов кафе баров ресторанов также жилых домов квартир подведение итогов ряда конкурсов вход бесплатный условии регистрации регистрации number рублей режим работы number number number number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_text = \"поздравляйте любимых оригинально в этом всегда поможет театр эстрады - поздравьте свою вторую половинку прямо со сцены театра! день всех влюблённых приглашаем провести, окунувшись в снежную историю любви для зрителей приготовили не просто концерт, а ещё и прекрасную возможность поздравить своих любимых с праздником прямо со сцены, а может признаться в любви или даже сделать предложение. в этот день все ваши желания и даже самые авантюрные поступки реальны! хотите поздравить свою вторую половинку? - заранее высылайте ваши поздравления/признания или другие интересные предложения для поздравления своих любимых на почту театра httpaddr с пометкой « number февраля», и они прозвучат со сцены в день концерта. не забудьте указать дату посещения. поторопитесь, ведь праздник не за горами! и не сомневайтесь, театр эстрады подготовил настоящий подарок в этот праздничный день всем влюблённым! а может, именно здесь вы встретите свою любовь\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = convert_text_to_feature_vector(not_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict([vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for text in not_events:\n",
    "    vector = convert_text_to_feature_vector(text)\n",
    "    prediction = classifier.predict([vector])\n",
    "    print(prediction[0] == 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data\\\\not_events\", \"r\", encoding=\"utf-8\") as f:\n",
    "    not_events = f.readlines()\n",
    "\n",
    "with open(\"data\\\\not_events_2\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    for e in not_events:\n",
    "        if len(e.strip()) > 2:\n",
    "            f.write(e)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
